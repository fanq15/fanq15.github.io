<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="cye-nm"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="stylesheet" href="./jemdoc.css" type="text/css">
<title>Qi Fan</title>
</head>
<body ryt12610="1">

<div id="layout-content" style="margin-top:15px">

<table>
  <tbody>
    <tr>
      <td width="240">
        <img src="./fanqi.jpg" width="180">
      </td>
      <td width="670">
        <div id="toptitle">         
          <h1>Qi Fan (范琦) &nbsp; </h1>
        </div>
        <h3>Associate Professor (准聘副教授) </a></h3>
        <p>
          <a href="https://cs.nju.edu.cn/rl/" target="_blank">R&amp;L Group</a><br>
          <a href="https://is.nju.edu.cn/main.htm" target="_blank">School of Intelligence Science and Technology</a><br>
		  <a href="https://keysoftlab.nju.edu.cn/main.psp">State Key Laboratory for Novel Software Technology</a><br>
		  <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University, Suzhou Campus</a><br>
			<br>
		  Email: <a href="mailto:fanqi@nju.edu.cn"> fanqi@nju.edu.cn</a>; <a href="mailto:fanqics@gmail.com"> fanqics@gmail.com</a> </p>

		  [<a href="https://scholar.google.com.tw/citations?user=da23smAAAAAJ&hl=en"><span style="color:purple">Google Scholar</span></a>] <!--[<a href="https://github.com/WenbinLee"><span style="color:purple">Github</span></a>]
[<a href="https://github.com/RL-VIG"><span style="color:purple">Github-VIG</span></a>] -->
        </p>
      </td>
<td valign="top" width="315"><a href="https://cs.nju.edu.cn/rl/index.htm" target="_blank"><img height="70" src="rlgroup-2.jpg" width="315" border="0"></a></td>

    </tr>
  </tbody>
</table>





<h2>Biography</h2>
  <p>
  <td class="style1" valign="top" height="84">Currently I am a Tenure-Track Associate Professor of <a href="https://is.nju.edu.cn/main.htm" target="_blank"> School of Intelligence Science and Technology</a> in
		  <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University</a>.
		  <span class="norm"><br class="style1"></span>I am also  a member of 
		  <a href="https://cs.nju.edu.cn/rl/" target="_blank">R&L Group</a>, led by professor 
		  <a href="https://cs.nju.edu.cn/gaoyang/" target="_blank">Yang Gao</a>.
	  	  <span class="norm"><br class="style1"></span>I received NSFC Excellent Young Scientists Fund (Overseas) in 2024.</a>.<br>


		  <span class="norm"><br class="style1"></span>I received my B.Eng. degree in <a href="https://see.cumt.edu.cn/index.htm" target="_blank"> School of Electrical Engineering </a>in June 2015 from <a href="https://www.cumt.edu.cn/" target="_blank"> China University of Mining and Technology</a>.
		  <span class="norm"><br class="style1"></span>I received my Master degree in <a href="https://www.au.tsinghua.edu.cn/" target="_blank"> Department of Automation </a>in June 2018 from <a href="https://www.tsinghua.edu.cn/" target="_blank"> Tsinghua University</a>.
		  <span class="norm"><br class="style1"></span>I received my Ph.D. degree in <a href="https://cse.hkust.edu.hk/" target="_blank"> Department of Computer Science & Engineering </a> in October 2023 from <a href="https://hkust.edu.hk/" target="_blank"> The Hong Kong University of Science and Technology </a> under the supervision of Prof. <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/cktang" target="_blank"> Chi-Keung TANG </a> (IEEE Fellow) and Prof. <a href="https://yuwingtai.github.io/" target="_blank"> Yu-Wing TAI</a>.
  </p>
  </td>
		  
<p></p>

<h2>Research Interests</h2>
  <p>
  Research interests include <u>computer vision</u> and <u>machine learning</u>. <br>
  My research primarily explores visual perception generalization technologies under data or resource constraints, including few-shot learning (detection, segmentation, classification, etc.), and the efficient fine-tuning of foundational visual models.
  </p>
<p></p>




<h2>Positions</h2>
<ul>
<li>
<b>请想要申请读研/读博的同学首先读这个文档: [<a href="https://github.com/fanq15/fanq15.github.io/blob/main/students.md"><b>招生要求与学生培养</b></a>]</b></p>
</li>
<li>
<h4>招收 2026 级申请-考核制博士 1 人，欢迎与我联系！</h4></p>
</li>
<li>
<h4>招收 2027 级保研/推免的硕士研究生 3 人，直博生 2 人，欢迎与我联系！</h4></p>
</li>
<li>
<h4>常年招收硕士研究生和博士研究生、科研助理、科研实习生等，欢迎与我联系！</h4></p>
</li>
<li>
<strike>招收 2026 级保研/推免的硕士研究生 3 人，直博生 1 人，欢迎与我联系！（已招满）</strike></p>
</li>
<li>
<strike>招收 2025 级申请-考核制的博士研究生 2 人，欢迎与我联系！（已招满）</strike></p>
</li>
<li>
<strike>招收 2025 级保研/推免的硕士研究生 3 人，欢迎与我联系！（已招满）</strike></p>
</li>
<li>
<strike>招收 2024 级考研的硕士研究生 2 人，欢迎与我联系！（已招满）</strike></p>
</li>
</ul>


<h2>Preprints</h2>
<ul>

<li>
<b>Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation</b> [<a href="https://arxiv.org/pdf/2508.03334.pdf"><span>paper</span></a>]<br></p>
  Xunzhi Xiang, Yabo Chen, Guiyu Zhang, Zhongyu Wang, Zhe Gao, Quanming Xiang, Gonghu Shang, Junqi Liu, Haibin Huang, Yang Gao, Chi Zhang, <b>Qi Fan (Corresponding Author)</b>, Xuelong Li <br></p>
<em>arXiv preprint arXiv:2508.03334</em><br>
</li>


<li>
<b>Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation</b> [<a href="https://arxiv.org/pdf/2506.18226.pdf"><span>paper</span></a>]<br></p>
  Xunzhi Xiang, <b>Qi Fan (Corresponding Author)</b><br></p>
<em>arXiv preprint arXiv:2506.18226</em><br>
</li>

<li>
<b>Denoising Vision Transformer Autoencoder with Spectral Self-Regularization</b> [<a href="https://arxiv.org/pdf/2511.12633.pdf"><span>paper</span></a>]<br></p>
  Xunzhi_Xiang, Xingye Tian, Guiyu Zhang, Yabo Chen, Shaofeng Zhang, Xuebo Wang, Xin Tao, <b>Qi Fan (Corresponding Author)</b><br></p>
<em>arXiv preprint arXiv:2511.12633</em><br>
</li>

<li>
<b>VideoTIR: Accurate and Efficient Understanding for Long Videos with Tool-Integrated Reinforcement Learning</b> [Under Review]<br></p>
  Zhe Gao, Taifeng Chai, Shiyu Shen, Weinong Wang, Haotian Xu, Xing W, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao, Dacheng Tao <br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>Training-Free Open-Vocabulary Semantic Segmentation with Context Pyramid Refinement</b> [Under Review]<br></p>
  Jialei Chen, <b>Qi Fan (Corresponding Author)</b>, Zhenzhen Quan, Dongyue Li, Xu Zheng, Hiroshi Murase, Daisuke Deguchi<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>Retain and Adapt: Auto-Balanced Model Editing for Open-Vocabulary Object Detection under Domain Shifts</b> [Under Review]<br></p>
  Zixuan Duan, Fengyuan Lu, Xunzhi Xiang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>QPrompt-R1: Real-Time Reasoning for Domain Generalized Semantic Segmentation via Group-Relative Query Alignment</b> [Under Review]<br></p>
  Fengyuan Lu, Zixuan Duan, Xunzhi Xiang, Zhicheng Zhang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>Selective, Regularized, and Calibrated: Harnessing Vision Foundation Models for Cross-Domain Few-Shot Semantic Segmentation</b> [Under Review]<br></p>
  Junyuan Ma, Xunzhi Xiang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>SmartSAM: Segment Ambiguous Objects like Smart Annotators</b> [Under Review]<br></p>
  Zhe Gao, Shiyu Shen, Xunzhi Xiang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>BIP: Bi-level Information Transfer and Completion Prompting for Visual Recognition with Missing Modalities</b> [Under Review]<br></p>
  Haoran Fan, Xu Han, Xianglong Bao, Zheng Gao, <b>Qi Fan (Corresponding Author)</b>, Yang Song, Jiaojiao Jiang<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>SAME: Sparse and Anchored Model Editing for Heterogeneous Incremental Learning under Limited Data</b> [Under Review]<br></p>
  Zixuan_Duan, Zeyu Zhang, Fengyuan Lu, Shaofeng Zhang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>Prompt-Free Universal Region Proposal Network</b> [Under Review]<br></p>
  Qihong Tang, Chang-Han Liu, Shaofeng Zhang, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao <br></p>
<em>In Submission</em>, 2025 <br>
</li>

<li>
<b>Dual-View Adaptation of Perspective Depth Foundation Models for Generalizable Panorama Depth Estimation</b> [Under Review]<br></p>
  Laiyan Ding, <b>Qi Fan (Corresponding Author)</b>, RuiHang Liu, Jiwei Chen, Rui Huang <br></p>
<em>In Submission</em>, 2025 <br>
</li>
	
</ul>

			
<h2>Publications</h2>
<ul>

<li>
<b>Semantic-Centric Alignment for Zero-shot Panoptic Segmentation with Limited Data</b> [Accepted]<br></p>
  Jialei Chen, Daisuke Deguchi, Dongyue Li, Xu Zheng, Seigo Ito, Hiroshi Murase, <b>Qi Fan (Corresponding Author)</b><br></p>
<em>International Journal of Computer Vision (IJCV) </em>, 2025 <br>
</li>


<li>
<b>DON'T NEED RETRAINING: A Mixture of DETR and Vision Foundation Models for Cross-Domain Few-Shot Object Detection</b> [Accepted]<br></p>
  Changhan Liu, Xunzhi Xiang, Zixuan Duan, Wenbin Li, <b>Qi Fan (Corresponding Author)</b>, Yang Gao<br></p>
<em>Neural Information Processing Systems (NeurIPS) </em>, 2025 <br>
</li>
	

<li>
<b>Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</b> [<a href="https://arxiv.org/pdf/2504.21414.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Kai-Qi Liu, Nian Liu, Hisham Cholakkal, Rao Muhammad Anwer, Wenbin Li, Yang Gao<br></p>
<em>International Conference on Computer Vision (ICCV) </em>, 2025 <br>
</li>
	

<li>
<b>Robust Object Detection with Domain-Invariant Training and Continual Test-Time Adaptation</b> [<a href="https://link.springer.com/article/10.1007/s11263-025-02465-9"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Mattia Segu, Bernt Schiele, Dengxin Dai, Yu-Wing Tai, Chi-Keung Tang<br></p>
<em>International Journal of Computer Vision (IJCV) </em>, 2025 <br>
</li>


<li>
<b>Stable Segment Anything Model</b> [<a href="https://arxiv.org/abs/2311.15776"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Xin Tao, Lei Ke, Mingqiao Ye, Yuan Zhang, Pengfei Wan, Yu-Wing Tai, Chi-Keung Tang<br></p>
<em>International Conference on Learning Representations (ICLR) </em>, 2025 <br>
</li>

<li>
<b>Domain-Rectifying Adapter for Cross-domain Few-Shot Segmentation</b> [<a href="https://arxiv.org/pdf/2404.10322.pdf"><span>paper</span></a>]<br></p>
  Jiapeng Su, <b>Qi Fan (Co-First Author)</b>, Guangming Lu, Fanglin Chen, Wenjie Pei <br></p>
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2024 <br>
</li>

<li>
<b>FSODv2: A Deep Calibrated Few-Shot Object Detection Network</b> [<a href="https://link.springer.com/article/10.1007/s11263-024-02049-z"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Wei Zhuo, Chi-Keung Tang, Yu-Wing Tai <br></p>
<em>International Journal of Computer Vision (IJCV) </em>, 2024 <a>(CVPR2020 extension)</a> <br>
</li>

<li>
<b>Learning to Learn Better Visual Prompts</b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28343"><span>paper</span></a>]<br></p>
  Fengxiang Wang, Wanrong Huang, Shaowu Yang, <b>Qi Fan</b>, Long Lan <br></p>
<em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) </em>, 2024 <br>
</li>

<li>
<b>GCoNet+: A Stronger Group Collaborative  Co-Salient Object Detector</b> [<a href="https://arxiv.org/pdf/2205.15469.pdf"><span>paper</span></a>]<br></p>
  Peng Zheng, Huazhu Fu, Deng-Ping Fan, <b>Qi Fan</b>, Jie Qin, Yu-Wing Tai, Chi-Keung Tang, Luc Van Gool <br></p>
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) </em>, 2023 <a>(CVPR2021 extension)</a> <br>
</li>

<li>
<b>Towards Robust Object Detection Invariant to Real-World Domain Shifts</b> [<a href="https://openreview.net/pdf?id=vqSyt8D3ny"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung Tang, Bernt Schiele, Dengxin Dai <br></p>
<em>International Conference on Learning Representations (ICLR) </em>, 2023 <br>
</li>

<li>
<b>Self-Support Few-Shot Semantic Segmentation</b> [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136790689.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Wenjie Pei, Yu-Wing Tai, Chi-Keung Tang <br></p>
<em>European Conference on Computer Vision (ECCV) </em>, 2022 <br>
</li>

<li>
<b>Few-Shot Video Object Detection</b> [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800071.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Chi-Keung Tang, Yu-Wing Tai <br></p>
<em>European Conference on Computer Vision (ECCV) </em>, 2022 <br>
</li>

<li>
<b>Few-Shot Object Detection with Model Calibration</b> [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136790707.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Chi-Keung Tang, Yu-Wing Tai <br></p>
<em>European Conference on Computer Vision (ECCV) </em>, 2022 <br>
</li>

<li>
<b>Group Collaborative Learning for Co-Salient Object Detection</b> [<a href="https://arxiv.org/pdf/2104.01108.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Deng-Ping Fan, Huazhu Fu, Chi-Keung Tang, Ling Shao, Yu-Wing Tai <br></p>
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2021 <br>
</li>

<li>
<b>Commonality-Parsing Network across Shape and Appearance for Partially Supervised Instance Segmentation</b> [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530375.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Lei Ke, Wenjie Pei, Chi-Keung Tang, Yu-Wing Tai <br></p>
<em>European Conference on Computer Vision (ECCV) </em>, 2020 <br>
</li>

<li>
<b>Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</b> [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Few-Shot_Object_Detection_With_Attention-RPN_and_Multi-Relation_Detector_CVPR_2020_paper.pdf"><span>paper</span></a>]<br></p>
  <b>Qi Fan</b>, Wei Zhuo, Chi-Keung Tang, Yu-Wing Tai <br></p>
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2020 <br>
</li>

</ul>


<h2>Awards and Honors</h2>
<ul>
<li>2024 ECCV 2nd workshop on Vision-based InduStrial InspectiON (VISION) Data Challenge - One Shot Industrial Defect Segmentation - 2nd place </li>
<li>2016 China Collegiate Computing Contest Big Data Challenge - Champion</li>
<!-- <li>2014 National Endeavor Scholarship</li>
<li>2012 Excellent Student Scholarship of China University of Mining and Technology</li> -->


</small></body></html>












